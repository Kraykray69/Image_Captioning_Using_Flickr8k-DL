{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce6352f-00f1-441d-b794-ade1911f5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rupes\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\rupes\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rupes\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: keras in c:\\users\\rupes\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\rupes\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\rupes\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: gradio in c:\\users\\rupes\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: absl-py in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: click in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.115.2)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (2.5.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.6.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio) (0.32.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio-client==1.4.0->gradio) (2024.3.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from gradio-client==1.4.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from fastapi<1.0->gradio) (0.40.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.25.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rupes\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib keras tensorflow nltk gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee12fbe-acd1-4729-aaa3-fcd165012a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555d3c71-451e-412d-acd5-7df4c855fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Embedding, LSTM, add\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.sparse\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79419674-dd72-43df-b2b7-b9a7fd3a9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files():\n",
    "    with zipfile.ZipFile('Flickr8k_Dataset.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('Flickr8k_Dataset')\n",
    "    with zipfile.ZipFile('Flickr8k_text.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('Flickr8k_text')\n",
    "\n",
    "def select_random_images(image_directory, num_images=500):\n",
    "    all_images = [img for img in os.listdir(image_directory) if img.endswith('.jpg')]\n",
    "    selected_images = random.sample(all_images, num_images)\n",
    "    selected_images_with_suffixes = [f\"{img}#{i}\" for img in selected_images for i in range(5)]\n",
    "    return selected_images, selected_images_with_suffixes\n",
    "\n",
    "def load_captions(captions_file):\n",
    "    captions = pd.read_csv(captions_file, sep='\\t', header=None)\n",
    "    captions.columns = ['image', 'caption']\n",
    "    return captions\n",
    "\n",
    "def preprocess_captions(captions_df, selected_images_with_suffixes):\n",
    "    filtered_captions = captions_df[captions_df['image'].isin(selected_images_with_suffixes)]\n",
    "    filtered_captions['caption'] = filtered_captions['caption'].apply(lambda x: f\"<start> {x} <end>\")\n",
    "    return filtered_captions\n",
    "\n",
    "def load_and_preprocess_images(selected_images, image_directory):\n",
    "    processed_images = []\n",
    "    for img_name in selected_images:\n",
    "        img_path = os.path.join(image_directory, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path).convert('RGB').resize((128, 128))\n",
    "            img_array = np.array(img) / 255.0  # Normalize\n",
    "            processed_images.append(img_array)\n",
    "        else:\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "    return np.array(processed_images)\n",
    "\n",
    "def create_custom_cnn(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f1b607e-2c90-49a5-8468-02667830a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(filtered_captions, selected_images, processed_images):\n",
    "    # Add special tokens to the captions\n",
    "    filtered_captions['caption'] = filtered_captions['caption'].apply(lambda x: f\"<start> {x} <end>\")\n",
    "    \n",
    "    tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(filtered_captions['caption'])\n",
    "    \n",
    "    # Manually add <start> and <end> tokens if they're not already in the vocabulary\n",
    "    if '<start>' not in tokenizer.word_index:\n",
    "        tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "    if '<end>' not in tokenizer.word_index:\n",
    "        tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(filtered_captions['caption'])\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    X_caption = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    \n",
    "    X_image = np.repeat(np.array(processed_images), len(filtered_captions) // len(processed_images), axis=0)\n",
    "\n",
    "    y_caption = [seq[1:] for seq in sequences]  # Shift sequences by one\n",
    "    y_caption = pad_sequences(y_caption, maxlen=max_length, padding='post')\n",
    "    y_caption = np.array([to_categorical(seq, num_classes=vocab_size) for seq in y_caption])\n",
    "    \n",
    "    return X_image, X_caption, y_caption, vocab_size, max_length, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3383897-a779-4577-9da4-877b24008728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    inputs_image = Input(shape=(128, 128, 3))\n",
    "    cnn_model = create_custom_cnn((128, 128, 3))\n",
    "    features = cnn_model(inputs_image)\n",
    "\n",
    "    inputs_caption = Input(shape=(max_length,))\n",
    "    embedding = Embedding(vocab_size, 256, mask_zero=True)(inputs_caption)\n",
    "    lstm = LSTM(256, return_sequences=True)(embedding)\n",
    "\n",
    "    features = Dense(256)(features)\n",
    "    features = RepeatVector(max_length)(features)\n",
    "    \n",
    "    decoder = add([features, lstm])\n",
    "    decoder = Dense(256, activation='relu')(decoder)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "    model = Model([inputs_image, inputs_caption], outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eca3b4b-cd11-412d-bcf8-32988ec9d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_image, X_caption, y_caption, vocab_size):\n",
    "    model.fit(\n",
    "        [X_image, X_caption], y_caption,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920b3059-4121-4d56-b864-20cce5a43246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface(model, tokenizer, max_length):\n",
    "    def predict_caption(image):\n",
    "        img_array = np.array(image.resize((128, 128))) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        input_seq = [tokenizer.word_index['<start>']]\n",
    "        for _ in range(max_length):\n",
    "            sequence = pad_sequences([input_seq], maxlen=max_length, padding='post')\n",
    "            pred = model.predict([img_array, sequence])\n",
    "            pred = np.argmax(pred[0], axis=-1)\n",
    "            word = tokenizer.index_word.get(pred[-1], '<unknown>')\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            input_seq.append(pred[-1])\n",
    "        \n",
    "        return ' '.join([tokenizer.index_word.get(idx, '<unknown>') for idx in input_seq[1:]])\n",
    "\n",
    "    iface = gr.Interface(fn=predict_caption, inputs=\"image\", outputs=\"text\")\n",
    "    return iface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ece760-25e0-43f2-afd8-2701b99529fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 616ms/step - loss: 3.0365 - val_loss: 1.6584\n",
      "Epoch 2/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 557ms/step - loss: 1.5539 - val_loss: 1.5296\n",
      "Epoch 3/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 554ms/step - loss: 1.4200 - val_loss: 1.4622\n",
      "Epoch 4/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 459ms/step - loss: 1.3313 - val_loss: 1.3747\n",
      "Epoch 5/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 481ms/step - loss: 1.2199 - val_loss: 1.3481\n",
      "Epoch 6/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 555ms/step - loss: 1.1331 - val_loss: 1.3342\n",
      "Epoch 7/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 489ms/step - loss: 1.0652 - val_loss: 1.3638\n",
      "Epoch 8/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 546ms/step - loss: 0.9351 - val_loss: 1.4140\n",
      "Epoch 9/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 613ms/step - loss: 0.8036 - val_loss: 1.4591\n",
      "Epoch 10/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 538ms/step - loss: 0.6916 - val_loss: 1.5483\n",
      "Epoch 11/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 584ms/step - loss: 0.6076 - val_loss: 1.5168\n",
      "Epoch 12/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 584ms/step - loss: 0.5679 - val_loss: 1.5949\n",
      "Epoch 13/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 491ms/step - loss: 0.5080 - val_loss: 1.6220\n",
      "Epoch 14/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 490ms/step - loss: 0.4705 - val_loss: 1.6550\n",
      "Epoch 15/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 436ms/step - loss: 0.4329 - val_loss: 1.6955\n",
      "Epoch 16/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 616ms/step - loss: 0.4147 - val_loss: 1.6926\n",
      "Epoch 17/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 495ms/step - loss: 0.3924 - val_loss: 1.7320\n",
      "Epoch 18/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 448ms/step - loss: 0.3688 - val_loss: 1.7392\n",
      "Epoch 19/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 455ms/step - loss: 0.3413 - val_loss: 1.7319\n",
      "Epoch 20/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 447ms/step - loss: 0.3258 - val_loss: 1.7990\n",
      "Epoch 21/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 436ms/step - loss: 0.3130 - val_loss: 1.8161\n",
      "Epoch 22/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 551ms/step - loss: 0.2917 - val_loss: 1.8414\n",
      "Epoch 23/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 677ms/step - loss: 0.2750 - val_loss: 1.8638\n",
      "Epoch 24/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 651ms/step - loss: 0.2599 - val_loss: 1.9020\n",
      "Epoch 25/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 647ms/step - loss: 0.2506 - val_loss: 1.9350\n",
      "Epoch 26/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 647ms/step - loss: 0.2350 - val_loss: 1.9584\n",
      "Epoch 27/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 572ms/step - loss: 0.2262 - val_loss: 1.9857\n",
      "Epoch 28/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 644ms/step - loss: 0.2144 - val_loss: 2.0283\n",
      "Epoch 29/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 561ms/step - loss: 0.2108 - val_loss: 2.0345\n",
      "Epoch 30/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 506ms/step - loss: 0.1967 - val_loss: 2.0699\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "images_directory = \"Flickr8k_Dataset/Flicker8k_Dataset\"\n",
    "captions_file = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "\n",
    "# Unzip files if necessary\n",
    "# unzip_files()\n",
    "\n",
    "# Select random images\n",
    "selected_images, selected_images_with_suffixes = select_random_images(images_directory)\n",
    "\n",
    "# Load and preprocess captions\n",
    "captions_df = load_captions(captions_file)\n",
    "filtered_captions = preprocess_captions(captions_df, selected_images_with_suffixes)\n",
    "\n",
    "# Load and preprocess images\n",
    "processed_images = load_and_preprocess_images(selected_images, images_directory)\n",
    "\n",
    "# Prepare sequences\n",
    "X_image, X_caption, y_caption, vocab_size, max_length, tokenizer = prepare_sequences(filtered_captions, selected_images, processed_images)\n",
    "\n",
    "# Define and train the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "train_model(model, X_image, X_caption, y_caption, vocab_size)\n",
    "\n",
    "# Create Gradio interface\n",
    "gradio_interface = create_gradio_interface(model, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71e8000b-474a-430d-a35e-b575a10a1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_gradio_interface(model, tokenizer, max_length):\n",
    "    def predict_caption(image):\n",
    "        try:\n",
    "            # Ensure the image is in RGB format\n",
    "            img = Image.fromarray(image).convert('RGB')\n",
    "            img_array = np.array(img.resize((128, 128))) / 255.0\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "            input_seq = [tokenizer.word_index['<start>']]\n",
    "            for i in range(max_length):\n",
    "                sequence = pad_sequences([input_seq], maxlen=max_length, padding='post')\n",
    "                pred = model.predict([img_array, sequence], verbose=0)\n",
    "                pred = np.argmax(pred[0], axis=-1)\n",
    "                word = tokenizer.index_word.get(pred[i], '<unknown>')\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "                if word != '<start>' and word != '<unknown>':\n",
    "                    input_seq.append(pred[i])\n",
    "            \n",
    "            caption = ' '.join([tokenizer.index_word.get(idx, '<unknown>') for idx in input_seq[1:] if idx not in [tokenizer.word_index['start'], \n",
    "                                                                                                                   tokenizer.word_index['end']]])\n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    iface = gr.Interface(\n",
    "        fn=predict_caption,\n",
    "        inputs=gr.Image(type=\"numpy\"),\n",
    "        outputs=\"text\",\n",
    "        live=False\n",
    "    )\n",
    "    return iface\n",
    "\n",
    "# Create and launch the interface\n",
    "gradio_interface = create_gradio_interface(model, tokenizer, max_length)\n",
    "gradio_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6ccc8-bf16-4616-88ab-b25770e599e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a9599-4b0d-4dfe-88c7-3fa819d87946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
